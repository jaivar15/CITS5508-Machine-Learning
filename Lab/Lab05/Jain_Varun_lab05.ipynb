{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq7baO3Q-Whn"
   },
   "source": [
    "# <center>  CITS5508 ML Lab05 </center>\n",
    "\n",
    "Student Name: **Varun Jain** <br>\n",
    "Student Number: **21963986** <br>\n",
    "Due Date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Y2Ae5US-X6q",
    "outputId": "bc235438-e829-4926-b517-1b39e38cf384"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVLzOOAR-a4y",
    "outputId": "ebe1779b-d0d6-4eb8-c9e4-dc32f6b7c0ac"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/My\\ Drive/lab05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaBXSp3H-cNZ",
    "outputId": "5849f610-c1f9-4303-e6dc-be363447f23a"
   },
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5P4YT1h-Who"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxT-wC1n-Whp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQQqFetOtzDO"
   },
   "outputs": [],
   "source": [
    "#The following code provided by Dr Du Huynh, will load the dataset and split the dataset into the test and training set.\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BH23DMk-Whp"
   },
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ye-RUdBr-Whq"
   },
   "outputs": [],
   "source": [
    "# According to the CIFAR-10 website, the training set is split into five batches\n",
    "# stored in fives files. Each colour image has dimensions equal to 32 x 32 x 3. There are 10 classes.\n",
    "image_width, image_height, image_Nchannels = 32, 32, 3\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# The default values of all the arguments of the load_batch function have been\n",
    "# set for the CIFAR-10 dataset.\n",
    "X_train, y_train = DataLoader.load_batch('data_batch')\n",
    "X_test, y_test = DataLoader.load_batch('test_batch', Nbatches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24Z3z-3l-Whr"
   },
   "source": [
    "# Data Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zRqMfYa-Whr"
   },
   "source": [
    "## Training and Test Set\n",
    "The dataset is split into a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxB91vhL-Whr",
    "outputId": "2fa68e3c-5bf6-447e-8c51-9b40446bea72"
   },
   "outputs": [],
   "source": [
    "train_set_size = X_train.shape[0]\n",
    "test_set_size = X_test.shape[0]\n",
    "print('Number of training instances:', train_set_size)\n",
    "print('Number of test instances:    ', test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E79HKurK-Whs"
   },
   "source": [
    "## Training and Validation Set\n",
    "The training set is broken down into a training set and validation set. The validation set contains 20% of the original training set data, while the remaining training instances are allocated to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYwhIuCP-Whs"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DPkFSF8-Whs",
    "outputId": "4187952e-9023-4121-bde4-7161c2a01a78"
   },
   "outputs": [],
   "source": [
    "train_set_size = x_train.shape[0]\n",
    "validation_set_size = x_val.shape[0]\n",
    "print('Number of training instances:  ', train_set_size)\n",
    "print('Number of validation instances:', test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc1bPcDL-Whs"
   },
   "source": [
    "# Generate 20 Random Images\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b1AwXWm-Wht"
   },
   "outputs": [],
   "source": [
    "#The random_20_images function returns 20 random images with their class labels\n",
    "\n",
    "def random_20_images(x_dataset, y_dataset, class_names):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for i in range(20):\n",
    "        #plot the images side by side\n",
    "        plt.subplot(5,4,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)     \n",
    "        #random number choosen that refers to an index value in the dataset\n",
    "        random_image_index = random.randint(0,len(x_dataset)-1)\n",
    "        #finds the image in the dataset with the index randomly specified \n",
    "        image_x_dataset = x_dataset[random_image_index]\n",
    "        #returns the class names for the image \n",
    "        class_name_index = class_names[y_dataset[random_image_index]]\n",
    "        #plots the images \n",
    "        plt.imshow(image_x_dataset, cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_name_index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "w7zr7V0--Wht",
    "outputId": "fbfba4ac-8fee-40fa-a1eb-e8bc240852a6"
   },
   "outputs": [],
   "source": [
    "#generates random 20 images for the training set \n",
    "random_20_images(x_train, y_train, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "KtQjNPFujPme",
    "outputId": "48cf12f2-17df-4e82-cbff-e26b8aceccfc"
   },
   "outputs": [],
   "source": [
    "#generates random 20 images for the validation set \n",
    "random_20_images(x_val, y_val, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "D3-CEmWyoZg0",
    "outputId": "cc9063d7-a7a8-43a2-efad-25b4083a24d0"
   },
   "outputs": [],
   "source": [
    "#generates random 20 images for the test set \n",
    "random_20_images(X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VwQErWv-Wht"
   },
   "source": [
    "# Multilayer Percerption Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooHbUHa5-Whw"
   },
   "source": [
    "## Training Model with Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQuhJ_UpxHQB"
   },
   "source": [
    "For the multilayer percerption model, the following hyperparamters: connection weight initialisation, learning rate scheduling and dropout rate has been tested with two experimental settings. For all my MLP models, each hyperparameter has been trained indiviual with two distinct hyperparameter values. The hyperparameter value that performs the best will be selected for the final architecture of the MLP model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKBdYCXL-Whw"
   },
   "source": [
    "### Connection Weight Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yYqHZulxHQC"
   },
   "source": [
    "For the connection weight initialisation, the following settings were explored: **glorot_uniform** and **he_uniform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3Gy7AeV-Whw"
   },
   "outputs": [],
   "source": [
    "# def connection_weight(connection_weight_initalisation):\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "#     tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=connection_weight_initalisation),\n",
    "#     tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=connection_weight_initalisation),\n",
    "#     tf.keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=connection_weight_initalisation)])\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    \n",
    "#     model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))\n",
    "    \n",
    "#     test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "#     return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91AGnJTH-Whx"
   },
   "outputs": [],
   "source": [
    "# model_glorot_uniform_loss, model_glorot_uniform_accuracy = connection_weight(\"glorot_uniform\")\n",
    "# print(f\"glorot_uniform_loss: {model_glorot_uniform_loss}, glorot_uniform_accuracy: {model_glorot_uniform_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36eRGtOe-Whx"
   },
   "outputs": [],
   "source": [
    "# model_he_uniform_loss, model_he_uniform_accuracy = connection_weight(\"he_uniform\")\n",
    "# print(f\"he_uniform_loss: {model_he_uniform_loss}, he_uniform_accuracy: {model_he_uniform_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "GrHccTFLxHQD",
    "outputId": "962d2388-8090-4eca-d29d-d9491828a7c0"
   },
   "outputs": [],
   "source": [
    "data=[[\"glorot_uniform\", 0.3644, 0.8722,  2.1881, 0.5130],[\"he_uniform\", 0.3564, 0.8786, 2.1841, 0.5101]]\n",
    "  \n",
    "\n",
    "df = pd.DataFrame(data, columns=['Connection Weight', 'Training Loss', 'Training Accuracy', 'Val Loss', 'Val Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmXYoBfsxHQE"
   },
   "source": [
    "Both connection weight initalizers: glorot_uniform and he_uniform ran for 100 epochs and the result table above shows how well the model performed with these settings. The glorot_uniform would be the optimal hyperparameter value. Even though the difference between the two settings are neligable, the glorort_uniform has a slightly better accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2Xr_tiC-Whx"
   },
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uQPJUCXxHQF"
   },
   "source": [
    "For the Learning Rate Schedule, the following settings were explored: **exponential learning rate** and **piecewise learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uweh5xyW-Why"
   },
   "outputs": [],
   "source": [
    "# def exponential_decay(lr0, s):\n",
    "#     def exponential_decay_fn(epoch):\n",
    "#         return lr0 * 0.1**(epoch / s)\n",
    "#     return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVDtnafN-Why"
   },
   "outputs": [],
   "source": [
    "# def create_optimizer_model_exponential(optimizer):\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "#     tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    \n",
    "#     lr_scheduler = keras.callbacks.LearningRateScheduler(optimizer)\n",
    "#     model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=[lr_scheduler])\n",
    "    \n",
    "#     test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "#     return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy2tFYT2-Why"
   },
   "outputs": [],
   "source": [
    "# exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "# exponential_loss, exponential_accuracy = create_optimizer_model_exponential(exponential_decay_fn)\n",
    "# print(f\"Explonential_Loss: {exponential_loss}, Exponential_accuracy: {exponential_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BTOho0n-Why"
   },
   "outputs": [],
   "source": [
    "# def piecewise_constant_fn(epoch):\n",
    "#     if epoch < 5:\n",
    "#         return 0.01\n",
    "#     elif epoch < 15:\n",
    "#         return 0.005\n",
    "#     else:\n",
    "#         return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvcQWOA3_VO8"
   },
   "outputs": [],
   "source": [
    "# def piecewise_constant(boundaries, values):\n",
    "#     boundaries = np.array([0] + boundaries)\n",
    "#     values = np.array(values)\n",
    "#     def piecewise_constant_fn(epoch):\n",
    "#         return values[np.argmax(boundaries > epoch) - 1]\n",
    "#     return piecewise_constant_fn\n",
    "\n",
    "# piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVuIx8Eg-Why"
   },
   "outputs": [],
   "source": [
    "# def create_optimizer_model_piecewise(piecewise_constant_fn):\n",
    "#     lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "#     tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "#     model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val),callbacks=[lr_scheduler])\n",
    "    \n",
    "#     test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "#     return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Cf1Kucb-Whz"
   },
   "outputs": [],
   "source": [
    "# piecewise_loss, piecewise_acc = create_optimizer_model_piecewise(piecewise_constant_fn)\n",
    "# print(f\"piecewise_loss:{piecewise_loss}, piecewise_acc:{piecewise_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "LiwOAg50xHQH",
    "outputId": "533c3283-5c31-4cc6-ca7b-3746f927f320"
   },
   "outputs": [],
   "source": [
    "data=[[\"Exponential\", 1.2250, 0.5775,  1.3855, 0.5119],[\"Piecewise\", 0.9727, 0.6673, 1.3632, 0.5297]]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Learning Rate Scheduling', 'Training Loss', 'Training Accuracy', 'Val Loss', 'Val Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwJ5oDtbxHQH"
   },
   "source": [
    "The Two Learning Rate Schedule: Exponential and Piecewise ran for the whole 100 epochs. From the table above, we can notice that the **Piecewise** learning rate performs slighly better. Judging by the results, both models are overfitting however the Piecewise seems to be overfitting but by a smaller amount. The optimal hyperparameter is **Piecewise** as it has a high accuracy and low loss for both training and validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2BdKqYb-Whz"
   },
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhPiW80kxHQI"
   },
   "source": [
    "For the **dropout** hyperparameter, the following two settings was observed: dropout value of 0.2 and 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGBBnMJh-Whz"
   },
   "outputs": [],
   "source": [
    "# def create_model_for_dropout_rate(dropout_rate):\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "#     tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "#     tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "#     tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "#     tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    \n",
    "#     model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))\n",
    "    \n",
    "#     test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "#     return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii57TB9g-Whz"
   },
   "outputs": [],
   "source": [
    "# dropout_rate_model_one = 0.2\n",
    "# dropout_rate_model_one_loss, dropout_rate_model_one_accuracy = create_model_for_dropout_rate(dropout_rate_model_one)\n",
    "# dropout_rate_model_two = 0.3\n",
    "# dropout_rate_model_two_loss, dropout_rate_model_two_accuracy = create_model_for_dropout_rate(dropout_rate_model_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T1A1aWi-Wh0"
   },
   "outputs": [],
   "source": [
    "# print(f\"dropout_rate_model_one_loss: {dropout_rate_model_one_loss},dropout_rate_model_one_accuracy{dropout_rate_model_one_accuracy}\")\n",
    "# print(f\"dropout_rate_model_two_loss:{dropout_rate_model_two_loss}, dropout_rate_model_two_accuracy:{dropout_rate_model_two_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "gJDt6HPDxHQJ",
    "outputId": "31795b5f-d5e4-4138-c139-0e1ca2fa361f"
   },
   "outputs": [],
   "source": [
    "data=[[\"0.2\", 1.2344,  0.5554,  1.3136 , 0.5336],[\"0.3\", 1.4242, 0.4915,  1.3608, 0.5139]]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Dropout Rate', 'Training Loss', 'Training Accuracy', 'Val Loss', 'Val Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emvLCPzIxHQJ"
   },
   "source": [
    "Model peforms better with a dropout rate of 0.2. The optimial setting is 0.2 because for both training and validation, loss and accuracy, the 0.2 dropout rate performs significantly better than dropout rate of 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaDmr_p2-Wh0"
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B48uh0sX-R5"
   },
   "source": [
    "For early stopping hyperparameter, the following two settings that was observed are 3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pe3rUMR-Wh0"
   },
   "outputs": [],
   "source": [
    "# def create_model_for_early_stopping(early_stopping_patience):\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "#     tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "#     early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stopping_patience)\n",
    "#     model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), callbacks=[early_stopping_cb])\n",
    "    \n",
    "#     test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "#     return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H-VOF5j-Wh0"
   },
   "outputs": [],
   "source": [
    "# early_stop_model_one_loss, early_stop_model_one_accuracy= create_model_for_early_stopping(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnIfqOrQ-Wh1"
   },
   "outputs": [],
   "source": [
    "# early_stop_model_two_loss, early_stop_model_two_accuracy= create_model_for_early_stopping(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwFS4mZr-Wh1"
   },
   "outputs": [],
   "source": [
    "# print(f\"early_stop_model_one_loss: {early_stop_model_one_loss}, early_stop_model_one_accuracy: {early_stop_model_one_accuracy}\")\n",
    "# print(f\"early_stop_model_two_loss: {early_stop_model_two_loss}, early_stop_model_two_accuracy: {early_stop_model_two_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "VRGFw0tP7Vc4",
    "outputId": "d94e5f00-813c-4839-ff54-eb09121e23df"
   },
   "outputs": [],
   "source": [
    "data=[[\"3\", 1.3832,  0.5179,  1.4972, 0.4688],[\"4\", 1.2764,  0.5590,  1.4257, 0.5014]]\n",
    "df = pd.DataFrame(data, columns=['Earling Stop', 'Training Loss', 'Training Accuracy', 'Val Loss', 'Val Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qBoT0t5X-R6"
   },
   "source": [
    "I found that a higher early stopping gave better training and validation, loss and accuracy. For the mlp model, the early stopping value 4 is the optimal hyperparameter value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rT9012X-Wh1"
   },
   "source": [
    "# Final Architecture: Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX2JsZvtvO70"
   },
   "source": [
    "The following hyperparameter,the optimal values found for the final architure were:\n",
    "- **Connection Weight Intialisation**: glorot_uniform\n",
    "- **Dropout Rate**: 0.2\n",
    "- **Learning Rate Curve**: Piecewise Learning Rate\n",
    "- **Early Stopping**: 4\n",
    "\n",
    "These optimal hyperparameter values are chosen for the final multilayer percerption model. These optimal values were found by testing each hyperparameter value individually. The reason i have trained my paramaters this way is beacuse gridsearchCV ran into some major issues while i was running. So, after talking to the lab demonstrator, he suggested that i run each hyperparameter value individually and design my final model based on the best values from my training model.\n",
    "\n",
    "\n",
    "Final Architecture Design: \n",
    " - My MLP Model consists of one input layer and output layer, and two dense hidden layers. The input layer is the Flatten() layer which flattens the input data. The first hidden layer has 300 neurons which progressively gets smaller to 100 neurons in the second hidden layer.  The design of the neurons in hidden layer helps the netural network to find the important information. \n",
    " - After each hidden layer, there is a dropout layer for regulisation purposes. It helps the model to reduce overfitting.\n",
    " - The activation function selected for my hidden layer is 'relu'. \n",
    " - For the output layer, i used the softmax activation function becuase it is used for multi-class classification the output later contains only 10 neurons because there are only 10 classes. \n",
    " - For the optimizer, i chose stochastic gradient descent (SGD) which has a predefined learning rate \n",
    " - The loss function for the MLP model was **SparseCategoricalCrossentropy** because it measures the performce of the multiclass classification problem. \n",
    " - For the Learning Rate Schedule, i use the Piecewise Constanst Decay. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIyDFyRq-Wh1"
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-tR3uwV-Wh3",
    "outputId": "b101148f-23e0-4647-9021-362d3fa44c95",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checks if the Jain_Varun_MLP directory exists\n",
    "if os.path.exists(\"Jain_Varun_MLP\"):\n",
    "  model_MLP = keras.models.load_model(\"Jain_Varun_MLP\")\n",
    "  model_MLP.summary()\n",
    "    # runs for one epoch\n",
    "  model_MLP.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val))\n",
    "else:\n",
    "    #directory Jain_Varun_MLP  doesnt exit\n",
    "  lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "  model_MLP = tf.keras.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "  tf.keras.layers.Dropout(rate=0.2),\n",
    "  tf.keras.layers.Dense(300, activation=\"relu\",kernel_initializer=\"glorot_uniform\"),\n",
    "  tf.keras.layers.Dropout(rate=0.2),\n",
    "  tf.keras.layers.Dense(100, activation=\"relu\",kernel_initializer=\"glorot_uniform\"),\n",
    "  tf.keras.layers.Dropout(rate=0.2),\n",
    "  tf.keras.layers.Dense(10, activation=\"softmax\",kernel_initializer=\"glorot_uniform\")])\n",
    "    \n",
    "  model_MLP.summary()\n",
    "\n",
    "  model_MLP.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    #run for 100 epochs\n",
    "  early_stopping_cb = keras.callbacks.EarlyStopping(patience=4)\n",
    "  checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Jain_Varun_MLP\", save_best_only=True)\n",
    "  history_mlp = model_MLP.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), callbacks=[checkpoint_cb, lr_scheduler, early_stopping_cb])\n",
    "\n",
    "MLP_Model_loss, MLP_Model_acc = model_MLP.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOcyvD8pxUha"
   },
   "source": [
    "The model is ran for 100 epoch. The table below shows the result of the training and validation sets, loss and accuracy rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "ds2JOyEQLX05",
    "outputId": "73246ba0-da32-42ea-c6ab-36b898277474"
   },
   "outputs": [],
   "source": [
    "data=[[\"glorot_uniform\", \"PieceWise\",\"0.2\",\"4\",  1.5495,  0.4506, 1.4945, 0.4717]]\n",
    "df = pd.DataFrame(data, columns=['Connection Weight','learning rate','dropout rate','Earling Stopping', 'Training Loss', 'Training Accuracy', 'Val Loss', 'Val Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8sKr1m-L9au"
   },
   "source": [
    "Observing the results, the models seems to be performing poorly as the accuracy rate for both the Training and Validation set is just below 50. There is no indication that the model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upJy85VcxUha"
   },
   "source": [
    "## Classification Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDjJcoA8xUhb"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "SZDOct0RqGZS",
    "outputId": "f7c48e12-f371-4c75-846d-02ea4b2d00e5"
   },
   "outputs": [],
   "source": [
    "y_pred_MLP_model = model_MLP.predict(X_test)\n",
    "y_pred_MLP_model = np.argmax(y_pred_MLP_model,axis=1)\n",
    "confusion_matrix_MLP_model = confusion_matrix(y_test, y_pred_MLP_model)\n",
    "heat_map = sns.heatmap(confusion_matrix_MLP_model, annot=True, xticklabels = class_names, yticklabels=class_names, fmt='', cmap='Blues')\n",
    "plt.title('MLP Confusion Matrix')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOqMsr8IxUhb"
   },
   "source": [
    "The confusion matrix above returns the performance of the multilayer perception model. From the confusion matrix, the following things can be said: \n",
    "- The diagonal from the top left to the bottom right represent the true positives. From observation, the model is performing relatively poorly. There is a large volume of instances where the model is correctly identifying the correct image, however there is also a large number of instances where the model incorrectly predicts the image.\n",
    "- The bird, cat, dog, and deer seems to have the highest misclassifcation rate. For example, if we look at class **cat**, out of the 952 images that were predicted, only 297 images got predicated correctly, and the remaining 655 images got classifed incorrectly with frog being the highest.\n",
    "- The automobile, airplane, along with ship and truck seem to be performing moderately well. For instance, for the class **ship**, out of the 1000 ship images, only 338 images were misclassifed. \n",
    "- Large volume of images especially from the animal class got misclassified as **frogs**\n",
    "- Noticeable trend: the probabiltiy that an image got misclassified and belongs to the same category, is relatively higher. For instance, in the vechicle classification, majority of the images are misclassified as another vehicle such as **airplane**. In numerical form,Airplane has been misclassified as Ship 182 times, and automobile has been misclassified as a truck 138 instances. This is evidently higher than the airplance being misclassified as a horse, in which only 32 instances got misclassified. This is the same for the animal category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeqlbCahxUhb"
   },
   "source": [
    "### Classification Accuracy and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMwBM5xxtIsL",
    "outputId": "479a2ba6-d62c-47e6-d241-09c39878e4df"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_MLP_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b61xvXduxUhc"
   },
   "source": [
    "Just for reference, \n",
    "class_names =<br> \n",
    "- [0: **'airplane'**, <br>\n",
    "- 1: **'automobile'**, <br>\n",
    "- 2: **'bird'**, <br>\n",
    "- 3: **'cat'**, <br>\n",
    "- 4: **'deer'**,<br>\n",
    "- 5: **'dog'**, <br>\n",
    "- 6: **'frog'**, <br>\n",
    "- 7:**'horse'**, <br>\n",
    "- 8: **'ship'**, <br>\n",
    "- 9: **'truck'**] where 0 represnts the index value given to airplane, and 9 to identify truck. <br> <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "- The overall accuracy is poor for the multilayer perception modelling, as it is just under 50%. The model does not perform well as the error rate is higher than the accuracy rate. \n",
    "- The weighted average percision is fairly low. \n",
    "- The fi-score for **bird**, **cat**, and **deer**, are **dog** are relatively low, which range from, 0.32 to 0.42. Overall, the model does not perform well for these classes as their f1-score is below 50. The **cat** class performs the worst and is the least accurate out all the other classes, with an f1 score of 32%. \n",
    "- The class **ship** has the highest f1-score. The f1 score for the class **ship** is 60%. \n",
    "- The classes with the highest f1-scores range from 0.51 to 0.61, which are the following classes: **airplane, automobile, frog, horse, ship, truck**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx7aqFpexUhd"
   },
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "KMVepWDHuKoM",
    "outputId": "97abf6db-0849-4a83-b09f-ec59935044b5"
   },
   "outputs": [],
   "source": [
    "def random_20_images_result(X_test, y_pred, y_test, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(20):\n",
    "        #plot the images side by side\n",
    "        plt.subplot(4,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)     \n",
    "        #random number choosen that refers to an index value in the dataset\n",
    "        random_image_index = random.randint(0,len(y_pred)-1)\n",
    "        if y_pred[random_image_index] == y_test[random_image_index]:\n",
    "          image_x_dataset = X_test[random_image_index]\n",
    "          class_name = class_names[y_test[random_image_index]]\n",
    "          plt.imshow(image_x_dataset, cmap=plt.cm.binary)\n",
    "          plt.title(class_name)\n",
    "        else:\n",
    "          image_x_dataset = X_test[random_image_index]\n",
    "          lass_name_actual = class_names[y_test[random_image_index]]\n",
    "          class_name = class_names[y_pred[random_image_index]]\n",
    "          plt.imshow(image_x_dataset, cmap=plt.cm.binary)\n",
    "          plt.title(\"Predict:\" + class_name + \" \\n \" +  \"Actual:\" + lass_name_actual, color=\"red\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "random_20_images_result(X_test,y_pred_MLP_model, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhHKcy7-xHQN"
   },
   "source": [
    "From the images above, we can note that the title with black labels are correctly classified and the titles with the red labels, are incorrectly classified. Higher percentage of the images misclassifed belong to the same cateory, for instance a deer is misclassified as a bird or a bird is classified as a deer. There is a lower percentage of classifications where bird is misclassified as truck (and vice versa). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gn-T2gi1Dcd"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyurYKRKxUhf"
   },
   "source": [
    "## Training Model with Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_gwRbmoxUhf"
   },
   "source": [
    "On the following CNN architecture, I ran eight distrinct test with the hyperparamaters to find out which combination of hyperparameters produce the best set. The following hyperparamters were tested: the kernal size, number of filters and activation function. For each hyperparametrs, i selected two values and ran them through the CNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pfZrwVx1Jfg"
   },
   "outputs": [],
   "source": [
    "# lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=[32, 32, 3]),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(64, (3,3),padding='same', activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "#   tf.keras.layers.BatchNormalization(), \n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(128, activation='relu'), \n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(64, activation='relu'), \n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(10, activation='relu')\n",
    "# ])\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(optimizer=\"sgd\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), callbacks=[lr_scheduler])\n",
    "# test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv6LrSbFxUhf"
   },
   "source": [
    "### Kernal Size\n",
    "\n",
    "For the CNN model the kernal size that were passed through the CNN model were three and five. In general, i found that the lower kernal size gave a better performance, in other words, it gave higher accurancy levels. Assuming ceteris paribus for all independent attribute except the kernal size, the kernal size of three generally performed better for each combination of attributes with the same number of kernals, and activation function but different kernal size. \n",
    "\n",
    "### Number of Kernals\n",
    "\n",
    "For the CNN model, the first convolutional layer started with either 32 or 64 filters. The filter size progressively got bigger for every convolution layer after the first layer. So the second convoultion layer has double the number of layers, and the third covolution has four times the number of layer from the first convolution layer. From the results, i found that that for all of my training models, the performance of the model with 64 filters than models with 32 filters, as 64 filters gave the model a higher accuracy output for both the training and validation set. \n",
    "\n",
    "### Activation Function \n",
    "\n",
    "The CNN model was tested with two activation functions: **relu** and **selu**. The model with the reLU activation function performed much better, as in for all cases, the accuracy on the validation set was greater than the ones outputted by the seLU activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ-_NnvIxUhg"
   },
   "source": [
    "From the table below, the first three columns represent the hyperparamters and the remaining columns represent the training and validation, loss and accuracy results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "aK0k7v4OYN4l",
    "outputId": "2b7f2042-a105-4d15-bd94-a3cd710ec362"
   },
   "outputs": [],
   "source": [
    "data=[[32,'relu',(3,3),0.1330,0.9558,1.0817,0.7478,''],\n",
    "      [64,'relu',(3,3),0.0324,0.9912,1.1199,0.7730,'Best'],\n",
    "      [32,'relu',(5,5),0.0393,0.9894,1.5177,0.7287,''],\n",
    "      [64,'relu',(5,5),0.0094,0.9980,1.3359,0.7680,''],\n",
    "      [32,'selu',(3,3),0.1430,0.9495,1.3205,0.7054,''],\n",
    "      [64,'selu',(3,3),0.0249,0.9931,1.5393,0.7164,''],\n",
    "      [32,'selu',(5,5),0.0317,0.9920,1.6921,0.6863,''],\n",
    "      [64,'selu',(5,5),0.0109,0.9979,1.4990,0.7086,'']]\n",
    "df = pd.DataFrame(data, columns=['Number of Kernals', 'activation_function', 'Kernals', 'training_loss', 'training_accuracy', 'validation_loss','validation_accuracy', 'Best'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC9BtbAhxUhh"
   },
   "source": [
    "The table above shows the expeiments i did to find the optimtial hyperparameters. The table shows the combinatation of the experiment and its results. Therefore, the optimal hyperparamater values are: \n",
    "- 64 number of kernals, \n",
    "- reLU for the activation function, and \n",
    "- (3,3) for kernal size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q58ubGeVYOVq"
   },
   "source": [
    "## Final Architecture: Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFu4K7iZxZ7F"
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw2XA-NixZqv"
   },
   "source": [
    "The CNN architecture consist of the following layers:\n",
    "- input layer, 3 convolution layer, batch normalisation layer, 3 max pooling layers, 2 hidden layers, 2 drop out layers and a output layer. \n",
    "- There is one max pooling layer after each convolution layer \n",
    "- The convolution layer has a padding, same\n",
    "- For each convolution layer, the number of filters doubled.\n",
    "- The activation function used is 'reLU'\n",
    "- the dropout rate of 0.2 and early stopping value of 4 were taken from the MLP model. There were the optimal values for the mlp model hyperparameters. \n",
    "- Batch normalisation and drop out are regularisation methods \n",
    "- The sgd optimizer was used in the CNN\n",
    "- The loss function: SparseCategoricalCrossentropy, basically computes the cross entropy between the actual and predicated labesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThyhzHAaxcjj",
    "outputId": "f114e65e-8349-4858-b89e-49dcffb5d28d"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"Jain_Varun_CNN\"):\n",
    "  model_CNN = keras.models.load_model(\"Jain_Varun_CNN\")\n",
    "  model_CNN.summary()\n",
    "  model_CNN.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val))\n",
    "else:\n",
    "  lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "  model_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=[32, 32, 3]),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3,3),padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'), \n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'), \n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "      \n",
    "  model_CNN.summary()\n",
    "\n",
    "  model_CNN.compile(optimizer=\"sgd\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "  early_stopping_cb = keras.callbacks.EarlyStopping(patience=4)\n",
    "  checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Jain_Varun_CNN\", save_best_only=True)\n",
    "  history_CNN = model_CNN.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), callbacks=[checkpoint_cb, lr_scheduler, early_stopping_cb])\n",
    "\n",
    "test_loss, test_acc = model_CNN.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CNN=[[64,'relu',(3,3),0.6027,0.7905,0.8067,0.7266]]\n",
    "df_CNN = pd.DataFrame(data_CNN, columns=['Number of Kernals', 'activation_function', 'Kernals', 'training_loss', 'training_accuracy', 'validation_loss','validation_accuracy'])\n",
    "df_CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyvB8h_mxPlT"
   },
   "source": [
    "## Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "yeYQ7iP3cGK2",
    "outputId": "f52c1a04-3d71-45f1-faac-778e83d07b6b"
   },
   "outputs": [],
   "source": [
    "y_pred_CNN = model_CNN.predict(X_test)\n",
    "y_pred_CNN = np.argmax(y_pred_CNN,axis=1)\n",
    "confusion_mx_CNN = confusion_matrix(y_test, y_pred_CNN)\n",
    "heat_map = sns.heatmap(confusion_mx_CNN, annot=True, xticklabels = class_names, yticklabels=class_names, fmt='', cmap='Blues')\n",
    "plt.title('CNN Confusion Matrix')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJxKtwJB1PWT"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "The confusion matrix above returns the performance of the convolutional neural network.\n",
    "- The diagonal (darker blue) is fairly dominent through out the entire matrix. From observation, the **bird** and **cat** class performed relatively much worse than the other class as the misclassifcation for the two classes is significantly higher.\n",
    "- High number of instances from the **bird** class has been misclassified as **cat, deer, dog, or frog**\n",
    "- High number of instances from the **cat** class has been misclassifed as **dog**, **deer** and **frog** with **dog** being the most dominent predication.\n",
    "- The **frog** class seems produces the higher number of instances with true positives, and overall the error rate for this class seems to be relatively lower.   \n",
    "- From the vechicle category, the **ship** class has the highest true positve value and has the lowest error rate.\n",
    "- Same trend as before, the misclassification of the class belonging to the same category is relatively much higher. This is evident in the confusion matrix, with **automobile** class being mislcassified as a **truck** 88 times, but only 21 times as a **frog**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "De7V4oyUdgO2",
    "outputId": "a435e6ef-f3bd-4053-c69e-e0376390d335"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_CNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT-x6rV51WB5"
   },
   "source": [
    "Just for reference, \n",
    "class_names =<br> \n",
    "- [0: **'airplane'**, <br>\n",
    "- 1: **'automobile'**, <br>\n",
    "- 2: **'bird'**, <br>\n",
    "- 3: **'cat'**, <br>\n",
    "- 4: **'deer'**,<br>\n",
    "- 5: **'dog'**, <br>\n",
    "- 6: **'frog'**, <br>\n",
    "- 7:**'horse'**, <br>\n",
    "- 8: **'ship'**, <br>\n",
    "- 9: **'truck'**] where 0 represnts the index value given to airplane, and 9 to identify truck. <br> <br> <br>\n",
    "\n",
    "- CNN performed moderately well with an accuracy of 72%. This means the model is able to identify the true positive and true negatives 72% of the time. \n",
    "- The weighted avergae for the precision, recall and fi-score is the same as the accuracy. \n",
    "- The class **cat** has the lowest fi score with a value of 50%.\n",
    "- The class **ship** and **automobile** have the highest f1-scores with 85% and 84% rectrospectively. \n",
    "- In terms of overall performance, the vehicle category performed better than the animal category as the highest f1-scores all belong to the vehicle category. \n",
    "- The animal category f1-score ranges from 50% to 78%\n",
    "- The vehicle category f1-score ranges from 77 to 85%. This simply could be due to the fact that the there are more classes for the animal category,hence the higher variabilty in accuracy for the animal class. \n",
    "- Overall, the performace of the CNN is fairly decent as just by looking at the f1-scores with majority of the f1-scores above 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "0i_aQFtzdlVg",
    "outputId": "ca690c9b-21b5-4fe9-ab36-f7fbe4c0e684"
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_20_images_result(X_test, y_pred, y_test, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(20):\n",
    "        #plot the images side by side\n",
    "        plt.subplot(4,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)     \n",
    "        #random number choosen that refers to an index value in the dataset\n",
    "        random_image_index = random.randint(0,len(y_pred)-1)\n",
    "        #select the images that got correctly classified \n",
    "        if y_pred[random_image_index] == y_test[random_image_index]:\n",
    "          image_x_dataset = X_test[random_image_index]\n",
    "          class_name = class_names[y_test[random_image_index]]\n",
    "          plt.imshow(image_x_dataset, cmap=plt.cm.binary)\n",
    "          plt.title(class_name)\n",
    "        else:\n",
    "            #select the images that got incorrectly classified\n",
    "          image_x_dataset = X_test[random_image_index]\n",
    "          lass_name_actual = class_names[y_test[random_image_index]]\n",
    "          class_name = class_names[y_pred[random_image_index]]\n",
    "          plt.imshow(image_x_dataset, cmap=plt.cm.binary)\n",
    "          plt.title(\"Predict:\" + class_name + \" \\n \" +  \"Actual:\" + lass_name_actual, color=\"red\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "random_20_images_result(X_test,y_pred_CNN, y_test, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkBodD0K5SOs"
   },
   "source": [
    "From the images above, we can note that the title with black labels are correctly classified and the titles with the red labels, are incorrectly classified. Higher percentage of the images misclassifed belong to the same cateory, for instance a deer is misclassified as a bird or a bird is classified as a deer or an automobile has been classified as a ship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00WMdwbidqE3"
   },
   "source": [
    "# Comparsion and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omg2RTLaX-SC"
   },
   "source": [
    "The following comparsions will be made between the MLP and CNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QqMSzT1xUhk"
   },
   "source": [
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmJl3DcIX-SC"
   },
   "source": [
    "- The CNN model accuracy is relatively much higher than the MLP model. From the current models produced, the CNN accuracy was 74% and the MLP's accuracy was 47%. For each class, you can see a significant improvement when image classification data is passed through the CNN model. \n",
    "- The F1-score for each class is higher in the CNN model than the MLP. \n",
    "- For both models, the vehicle category has the highest F1-scores. The top three classes belong to the vehicle category, **automobile**, **ship**, and **truck** ranging from 0.83 to 0.85 from the CNN model, and 0.53 to 0.59  from the MLP model. \n",
    "- For both models, the **cat** class has the lowest F1-score and the only class below 60% in the CNN model. \n",
    "- The lowest F1-scores belong to the animal category: **bird**, **cat**, **dog** classes. In the MLP model, the F1-score range from 0.32 and 0.36, and for the CNN model, the F1-score range from 0.50 to 0.64. \n",
    "- For the CNN model, there is a significant improvement in class 9, the **horse** class and **frog** class, as for both classes, the F1-score is above 70%. \n",
    "- Therefore, we can conlude that both models produce similar results in terms of class to class comparison, but as an overall, the accuracy and F1-score is significantly higher for the CNN model. \n",
    "<br>Notes may slightly varied based when the one epoch is ran. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHMHMzifxUhl"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZAlCM6DX-SC"
   },
   "source": [
    "- In comparsion of the two confusion matrix produced by the MLP and CNN, we can say that the performance of the CNN is relatively better as the total true positives for each class is much higher in the CNN, in comparsion to the MLP confusion matrix. For each class, the number of instances that have been misclassified is higher in the MLP model than the CNN model. Therefore, the CNN model produces a better accuracy when it comes to classifying the class of the image. \n",
    "- In both models, the class **cat** has commonly been misclassified as **dog** and **frog**. \n",
    "- In both models, the class **airplane** and **automobile** has commonly been misclassified as **ship** and **truck**, rectrospectively. \n",
    "- In the MLP model, **deer** has commonly been misclassified as **bird** and true for the CNN model but the magnitude for the misclassifiation error is much much lower. In this area, the CNN has significantly improve the clasification of **deer**. \n",
    "- In some cases, the CNN does not performed as well, as expected. In both models, the **bird** class gets misclassified to a **dog**. However, CNN model has a higher misclassifation rate for that particular class than the MLP. \n",
    "- The CNN is not perfect and even though it does produce a higher misclassification rate for some classes, overall, the CNN significantly improves the quality and performance of the image classification model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR4ZvM9SxUhl"
   },
   "source": [
    "### Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFLOtmyvX-SD"
   },
   "source": [
    "- My final MLP architecture has 7 layers with  953,010 training parameters. \n",
    "- My final CNN architecture has 13 layers with 904,650 trainable parameters and 512 non-trainable parameters. \n",
    "- The CNN model has less training parameters but nearly double the number of layers, and therefore increasing the complexity of the model. \n",
    "- The CNN model incorporates the the basic MLP model but on top, it includes regularization methods such as, batch normalisation and dropout rate, max poolling and three convolutional layers. In comparsion, the MLP model only consists of three input layer, few hidden layers and output layer, with dropout rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86mPg49NX-SD"
   },
   "source": [
    "### Training Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xcC4syHX-SD"
   },
   "source": [
    "To train the models, i used Google Colob with the GPU runtime. Overall, the CNN model took longer to run and this is due to the complexity of the model. The model training time for the MLP model took about 2 to 3 seconds per epoch, and for the CNN model, it took 6 seconds per epoch. Therefore, the CNN has a longer training time due to the complexity of the model. Without the GPU, the MLP took about 6 to 8 seconds per epoch and the CNN took about 3 to 4 minutes per epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1_SFkpHX-SD"
   },
   "source": [
    "For my final architecture, with the help of early stopping, the CNN ran faster than my MLP model, in terms of total time. The number of epochs that ran for the CNN model were significantly lower than the number of epochs that ran for the MLP, so in total time, CNN ran faster. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
